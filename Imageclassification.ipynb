{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN6IwEslJk1nPvJ3x3vWB9Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["1.What is a Convolutional Neural Network (CNN), and how does it differ from\n","traditional fully connected neural networks in terms of architecture and performance on\n","image data?\n","- A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed to process grid-like data, such as images. CNNs automatically learn spatial features (edges, textures, shapes, objects) by applying convolution operations using learnable filters.\n","\n","- A Fully Connected Neural Network (also called Multilayer Perceptron) connects every neuron in one layer to every neuron in the next.\n","\n","2.Discuss the architecture of LeNet-5 and explain how it laid the foundation\n","for modern deep learning models in computer vision. Include references to its original\n","research paper.\n","\n","- LeNet-5 is one of the earliest and most influential Convolutional Neural Networks (CNNs), proposed by Yann LeCun et al. in the late 1990s. It was primarily designed for handwritten digit recognition (such as ZIP code and bank cheque recognition) and laid the conceptual and architectural foundation for modern deep learning models in computer vision.\n","\n","- Yann LeCun, L√©on Bottou, Yoshua Bengio, Patrick Haffner (1998)\n","‚ÄúGradient-Based Learning Applied to Document Recognition‚Äù\n","\n","3.Compare and contrast AlexNet and VGGNet in terms of design principles,\n","number of parameters, and performance. Highlight key innovations and limitations of\n","each.\n","\n","- AlexNet and VGGNet are two landmark convolutional neural network architectures that significantly influenced the evolution of deep learning in computer vision. While both were designed for large-scale image classification on the ImageNet dataset, they differ notably in their design philosophy, complexity, and performance.\n","- AlexNet, introduced in 2012 by Alex Krizhevsky and colleagues, was the first deep CNN to demonstrate a dramatic improvement in image classification accuracy. Its architecture consists of eight layers, including five convolutional layers followed by three fully connected layers. AlexNet employs relatively large convolution filters such as 11√ó11 and 5√ó5 in the initial layers, which allow rapid reduction of spatial dimensions. A major design principle of AlexNet was to make deep learning feasible on available hardware, achieved through GPU-based training and architectural simplifications.\n","- VGGNet, proposed in 2014 by Simonyan and Zisserman, was designed with the principle that increasing network depth using small, consistent convolution filters leads to better performance. VGGNet uses only 3√ó3 convolution kernels stacked sequentially, which effectively increases the receptive field while keeping the architecture simple and uniform. Popular versions such as VGG-16 and VGG-19 contain 16 and 19 weight layers respectively, making them much deeper than AlexNet.\n","\n","4.What is transfer learning in the context of image classification? Explain\n","how it helps in reducing computational costs and improving model performance with\n","limited data.\n","- Transfer learning in the context of image classification is a deep learning technique in which a model trained on a large and general image dataset is reused for a new but related task. Instead of training a convolutional neural network (CNN) from scratch, a pretrained model such as VGG, ResNet, Inception, or MobileNet‚Äîalready trained on large datasets like ImageNet‚Äîis adapted to classify images from a new target dataset.\n","- In image classification, the early layers of a CNN learn generic visual features such as edges, corners, textures, and shapes, while the deeper layers learn task-specific features. Transfer learning leverages this property by reusing the learned weights of early layers and modifying or retraining only the final layers for the new classification problem.\n","\n","5.Describe the role of residual connections in ResNet architecture. How do\n","they address the vanishing gradient problem in deep CNNs?\n","- Residual connections are the core idea behind the ResNet (Residual Network) architecture, introduced by He et al. in 2015, and they enable the successful training of very deep convolutional neural networks (50, 101, or even 152 layers).\n","- Mathematically, instead of learning a function\n","ùêª\n","(\n","ùë•\n",")\n","H(x), the network learns\n","ùêπ\n","(\n","ùë•\n",")\n","=\n","ùêª\n","(\n","ùë•\n",")\n","‚àí\n","ùë•\n","F(x)=H(x)‚àíx, and the final output becomes\n","ùë¶\n","=\n","ùêπ\n","(\n","ùë•\n",")\n","+\n","ùë•\n","y=F(x)+x. This makes it easier for the network to learn identity mappings when deeper layers are not needed.\n","\n","6.Implement the LeNet-5 architectures using Tensorflow or PyTorch to\n","classify the MNIST dataset. Report the accuracy and training time."],"metadata":{"id":"iQvc8b_ESwd_"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"d83UifnjStFs","executionInfo":{"status":"error","timestamp":1767703620029,"user_tz":-330,"elapsed":20178,"user":{"displayName":"Tanmay Ghule","userId":"02288771655695874608"}},"outputId":"11fe6158-a8ea-49d5-e257-fc4c37faf423"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"error","ename":"TypeError","evalue":"AveragePooling2D.__init__() missing 1 required positional argument: 'pool_size'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2716779788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m model = models.Sequential([\n\u001b[1;32m     18\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: AveragePooling2D.__init__() missing 1 required positional argument: 'pool_size'"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import time\n","\n","# Load MNIST dataset\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","# Preprocessing\n","x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n","x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n","\n","# Pad images to 32x32 as required by LeNet-5\n","x_train = tf.pad(x_train, [[0,0],[2,2],[2,2],[0,0]])\n","x_test = tf.pad(x_test, [[0,0],[2,2],[2,2],[0,0]])\n","\n","# Build LeNet-5 model\n","model = models.Sequential([\n","    layers.Conv2D(6, kernel_size=5, activation='tanh', input_shape=(32,32,1)),\n","    layers.AveragePooling2D(),\n","\n","    layers.Conv2D(16, kernel_size=5, activation='tanh'),\n","    layers.AveragePooling2D(),\n","\n","    layers.Conv2D(120, kernel_size=5, activation='tanh'),\n","    layers.Flatten(),\n","\n","    layers.Dense(84, activation='tanh'),\n","    layers.Dense(10, activation='softmax')\n","])\n","\n","# Compile model\n","model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# Train model and measure time\n","start_time = time.time()\n","\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=10,\n","    batch_size=128,\n","    validation_split=0.1,\n","    verbose=1\n",")\n","\n","training_time = time.time() - start_time\n","\n","# Evaluate model\n","test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n","\n","print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n","print(f\"Training Time: {training_time:.2f} seconds\")\n"]},{"cell_type":"markdown","source":["7.Use a pre-trained VGG16 model (via transfer learning) on a small custom\n","dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model.\n","Include your code and result discussion."],"metadata":{"id":"dFlH4FfXWH35"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import time\n","\n","# Image parameters\n","IMG_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","NUM_CLASSES = 4\n","\n","# Data generators\n","train_gen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    zoom_range=0.2,\n","    horizontal_flip=True\n",")\n","\n","val_gen = ImageDataGenerator(rescale=1./255)\n","\n","train_data = train_gen.flow_from_directory(\n","    \"flowers/train\",\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode=\"categorical\"\n",")\n","\n","val_data = val_gen.flow_from_directory(\n","    \"flowers/validation\",\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode=\"categorical\"\n",")\n","\n","# Load pre-trained VGG16 (without top layers)\n","base_model = VGG16(\n","    weights=\"imagenet\",\n","    include_top=False,\n","    input_shape=(224, 224, 3)\n",")\n","\n","# Freeze base model\n","base_model.trainable = False\n","\n","# Custom classifier\n","model = models.Sequential([\n","    base_model,\n","    layers.Flatten(),\n","    layers.Dense(256, activation=\"relu\"),\n","    layers.Dropout(0.5),\n","    layers.Dense(NUM_CLASSES, activation=\"softmax\")\n","])\n","\n","# Compile model\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","    loss=\"categorical_crossentropy\",\n","    metrics=[\"accuracy\"]\n",")\n","\n","# Train model\n","start_time = time.time()\n","\n","history = model.fit(\n","    train_data,\n","    validation_data=val_data,\n","    epochs=10\n",")\n","\n","training_time = time.time() - start_time\n","\n","# Fine-tuning: unfreeze top VGG16 layers\n","base_model.trainable = True\n","\n","for layer in base_model.layers[:-4]:\n","    layer.trainable = False\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n","    loss=\"categorical_crossentropy\",\n","    metrics=[\"accuracy\"]\n",")\n","\n","history_finetune = model.fit(\n","    train_data,\n","    validation_data=val_data,\n","    epochs=5\n",")\n","\n","print(f\"Total Training Time: {training_time:.2f} seconds\")\n"],"metadata":{"id":"UbARe53IWwcW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8.Write a program to visualize the filters and feature maps of the first\n","convolutional layer of AlexNet on an example input image."],"metadata":{"id":"cLGWen3RW7Vf"}},{"cell_type":"code","source":["import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Load pretrained AlexNet\n","alexnet = models.alexnet(pretrained=True)\n","alexnet.eval()\n","\n","# Extract first convolutional layer\n","first_conv = alexnet.features[0]\n","\n","# -------------------------------\n","# 1. Visualize Filters (Kernels)\n","# -------------------------------\n","filters = first_conv.weight.data.clone()\n","\n","# Normalize for visualization\n","filters = (filters - filters.min()) / (filters.max() - filters.min())\n","\n","plt.figure(figsize=(8, 8))\n","for i in range(16):  # show first 16 filters\n","    plt.subplot(4, 4, i+1)\n","    kernel = filters[i]\n","    kernel = kernel.permute(1, 2, 0)  # CxHxW ‚Üí HxWxC\n","    plt.imshow(kernel)\n","    plt.axis(\"off\")\n","\n","plt.suptitle(\"AlexNet First Convolution Layer Filters\")\n","plt.show()\n","\n","# -------------------------------\n","# 2. Visualize Feature Maps\n","# -------------------------------\n","\n","# Load example image\n","image = Image.open(\"example.jpg\").convert(\"RGB\")\n","\n","# Preprocessing (AlexNet input format)\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","\n","input_tensor = transform(image).unsqueeze(0)\n","\n","# Forward pass through first conv layer\n","with torch.no_grad():\n","    feature_maps = first_conv(input_tensor)\n","\n","# Convert to numpy\n","feature_maps = feature_maps.squeeze(0)\n","\n","plt.figure(figsize=(8, 8))\n","for i in range(16):  # show first 16 feature maps\n","    plt.subplot(4, 4, i+1)\n","    plt.imshow(feature_maps[i], cmap=\"gray\")\n","    plt.axis(\"off\")\n","\n","plt.suptitle(\"Feature Maps from First Conv Layer\")\n","plt.show()\n"],"metadata":{"id":"7B1PrAGQXCJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["9.Train a GoogLeNet (Inception v1) or its variant using a standard dataset\n","like CIFAR-10. Plot the training and validation accuracy over epochs and analyze\n","overfitting or underfitting.\n"],"metadata":{"id":"nTrKsnBoXOZP"}},{"cell_type":"code","source":["def GoogLeNet_CIFAR10():\n","    inputs = layers.Input(shape=(32, 32, 3))\n","\n","    x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(inputs)\n","    x = layers.MaxPooling2D((2,2))(x)\n","\n","    x = inception_module(x, 32, 48, 64, 8, 16, 16)\n","    x = inception_module(x, 64, 64, 96, 16, 32, 32)\n","\n","    x = layers.MaxPooling2D((2,2))(x)\n","\n","    x = inception_module(x, 96, 64, 96, 16, 32, 32)\n","\n","    x = layers.GlobalAveragePooling2D()(x)\n","    x = layers.Dense(256, activation='relu')(x)\n","    x = layers.Dropout(0.4)(x)\n","\n","    outputs = layers.Dense(10, activation='softmax')(x)\n","\n","    return models.Model(inputs, outputs)\n","\n","model = GoogLeNet_CIFAR10()\n","model.summary()\n"],"metadata":{"id":"1V-ITFUQXTP-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["10.You are working in a healthcare AI startup. Your team is tasked with\n","developing a system that automatically classifies medical X-ray images into normal,\n","pneumonia, and COVID-19. Due to limited labeled data, what approach would you\n","suggest using among CNN architectures discussed (e.g., transfer learning with ResNet\n","or Inception variants)? Justify your approach and outline a deployment strategy for\n","production use\n","\n","-  Recommended Approach\n","- Model Choice Justification\n","- Data Preparation\n","- Explainability\n","- Continuous Learning in Production\n","-"],"metadata":{"id":"ydCPpn6lYx2O"}},{"cell_type":"code","source":[],"metadata":{"id":"zNQ1ouYmY3hq"},"execution_count":null,"outputs":[]}]}