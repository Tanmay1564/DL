{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1: What is Detectron2 and how does it differ from previous object detection\n","frameworks?\n","- Detectron2 provides state-of-the-art implementations of popular computer vision models, including:\n","\n"," - It is widely used in research and production for tasks like image understanding, medical imaging, autonomous driving, and surveillance.\n","\n","How Detectron2 differs from previous object detection frameworks\n","1. PyTorch-based (vs Caffe2 in Detectron)\n","\n","Detectron2 is built entirely on PyTorch\n","\n","Enables dynamic computation graphs\n","\n","Easier debugging, customization, and experimentation\n","\n","Older Detectron used Caffe2, which was harder to modify and debug.\n","\n","2. Modular and Flexible Architecture\n","\n","Components such as backbones, heads, losses, and datasets are highly modular\n","\n","Easy to plug in custom models or datasets\n","\n"," Earlier frameworks were more monolithic and less flexible.\n","\n","3. Improved Performance & Scalability\n","\n","Optimized training and inference pipelines\n","\n","Supports multi-GPU and distributed training\n","\n","Faster experimentation and deployment\n","\n","4. Better Dataset Handling\n","\n","Native support for datasets like COCO, LVIS, and Pascal VOC\n","\n","Simple APIs for registering custom datasets\n","\n"," Older frameworks required more manual dataset configuration.\n","\n","5. Production-Ready Features\n","\n","TorchScript support for model deployment\n","\n","Better checkpointing and reproducibility\n","\n","Clean integration with modern PyTorch tools\n","\n","6. State-of-the-Art Research Support\n","\n","Frequently updated with latest research models\n","\n","Strong benchmark results on COCO and other datasets\n","\n"," Detectron2 is actively maintained, unlike many older frameworks."],"metadata":{"id":"dLk5_7cTNNXE"}},{"cell_type":"markdown","source":["2. Explain the process and importance of data annotation when working with\n","Detectron2.\n","- Step 1: Define the Task\n","\n","Decide what the model needs to learn:\n","\n","Object detection → Bounding boxes + class labels\n","\n","Instance segmentation → Pixel-level masks\n","\n","Keypoint detection → Body/joint points\n","\n","Step 2: Choose Annotation Format\n","\n","Detectron2 commonly supports:\n","\n","COCO JSON format\n","\n","Pascal VOC XML\n","\n","Custom datasets (registered via Detectron2 API)\n","\n","Step 3: Annotate the Data\n","\n","Use annotation tools such as:\n","\n","LabelImg (bounding boxes)\n","\n","CVAT\n","\n","LabelMe\n","\n","Roboflow\n","\n","Annotations include:\n","\n","Class name (e.g., person, car)\n","\n","Bounding box coordinates\n","\n","Segmentation masks (if required)\n","\n","Step 4: Quality Checking\n","\n","Remove incorrect labels\n","\n","Ensure consistency in class names\n","\n","Validate box/mask alignment\n","\n","Poor-quality annotations lead to poor model performance.\n","\n","Step 5: Dataset Registration\n","\n","Register annotated datasets in Detectron2\n","\n","Split data into training, validation, and testing sets\n","\n","2. Importance of Data Annotation\n","1. Enables Supervised Learning\n","\n","Detectron2 models rely on labeled data to learn object features.\n","Without annotation → model cannot be trained.\n","\n","2. Directly Impacts Model Accuracy\n","\n","Accurate labels → high precision & recall\n","\n","Incorrect labels → false detections, poor generalization\n","\n","\n","\n","3. Supports Advanced Tasks\n","\n","Proper annotation enables:\n","\n","Instance segmentation\n","\n","Panoptic segmentation\n","\n","Keypoint detection\n","\n","These tasks cannot work with weak or incomplete labels.\n","\n","4. Reduces Training Time and Overfitting\n","\n","Clean annotations:\n","\n","Help models converge faster\n","\n","Reduce noise in loss functions\n","\n","5. Ensures Real-World Reliability\n","\n","For applications like:\n","\n","Medical imaging\n","\n","Autonomous driving\n","\n","Surveillance\n","\n","Incorrect annotations can cause serious real-world failures."],"metadata":{"id":"AJ99L7KWOqft"}},{"cell_type":"markdown","source":["3. Describe the steps involved in training a custom object detection model\n","using Detectron2.\n","- Install Detectron2\n","- Prepare and Annotate the Dataset\n","- Register the Dataset in Detectron2\n","- Choose a Pretrained Model & Configuration\n","- Modify Configuration for Custom Training\n","- Start Training the Model\n","- Evaluate the Model\n","- Run Inference on New Images\n","- Save/Deploy the Model"],"metadata":{"id":"Z4ct60joQkql"}},{"cell_type":"markdown","source":["4. What are evaluation curves in Detectron2, and how are metrics like mAP\n","and IoU interpreted?\n","- Evaluation Curves in Detectron2\n","   - Evaluation curves are graphical tools used to understand the performance of an object detection model. These curves help visualize:\n","      - How well the model detects object\n","      - How accurate the predictions are\n","      - Trade-off between precision and recall\n","      - Performance across IoU thresholds\n","      1. Precision–Recall (PR) Curve\n","      2. IoU–Accuracy Curve\n","      3. Loss Curves\n","- 1. IoU (Intersection over Union)\n","    - IoU measures how well a predicted bounding box overlaps with the ground truth (actual object).\n","    - IoU=Area of Union/Area of Overlap​\n","    - Interpretation\n","       - IoU = 1.0 → Perfect match\n","       - IoU = 0.5 → 50% overlap\n","       - IoU < 0.5 → Often considered a wrong detection\n","      - Higher IoU = better localization accuracy\n","2. mAP (mean Average Precision)\n","   - mAP measures the overall performance of the model in detecting and classifying objects across multiple IoU thresholds and classes.\n","   - How mAP Works\n","      - Compute Precision–Recall curve for each class\n","      - Compute Average Precision (AP) for each class\n","      - Take the mean of AP across all classes and IoU thresholds\n","    - Interpretation\n","       - Higher mAP = better overall detection accuracy\n","       - AP50 is easier to achieve, AP75 is more strict\n","       - AP@[0.50:0.95] is the most reliable metric"],"metadata":{"id":"fii9PQoARVPb"}},{"cell_type":"markdown","source":["5. Compare Detectron2 and TFOD2 in terms of features, performance, and\n","ease of use.\n","- Detectron2\n","   - 1. Framework & Backend\n","     - Built on PyTorch\n","     - Modern, modular architecture\n","     - Designed by Facebook AI Research (FAIR)\n","   - 2. Features\n","      - Object Detection\n","      - Instance Segmentation\n","      - Panoptic Segmentation\n","      - Keypoint Detection\n","      - DensePose\n","    - 3. Model Zoo\n","      - Faster R-CNN\n","      - Mask R-CNN\n","      - Cascade R-CNN\n","      - RetinaNet\n","      - Panoptic FPN\n","    - 4. Performance\n","      - Faster training due to efficient PyTorch pipelines\n","      - High accuracy for segmentation and complex tasks\n","      - Excellent multi-GPU support\n","- TFOD2\n","   - 1. Framework & Backend\n","       - Built on TensorFlow\n","       - Developed by Google\n","       - Optimized for production and TPU support\n","    - 2. Features\n","       - Object Detection\n","       - Instance Segmentation (limited models)\n","       - Uses pipeline config files for training\n","    - 3. Model Zoo\n","        - Wide range of production models\n","        - SSD\n","        - EfficientDet\n","        - Faster R-CNN\n","        - CenterNet\n","        - MobileNet-SSD\n","    4. Performance\n","        - Optimized for TPU and mobile deployment\n","        - EfficientDet gives fast inference\n","        - Very competitive accuracy for detection tasks"],"metadata":{"id":"V47kt_DKTPRF"}},{"cell_type":"markdown","source":["6. Write Python code to install Detectron2 and verify the installation.\n"],"metadata":{"id":"8EijxN8-V9kJ"}},{"cell_type":"code","source":["# Import Detectron2 to verify installation\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","\n","setup_logger()\n","\n","print(\"Detectron2 successfully installed!\")\n","print(\"Version:\", detectron2.__version__)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tX_ZrUHWf3S","outputId":"ba56218a-4558-42a8-bbff-a276e91993e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detectron2 successfully installed!\n","Version: 0.6\n"]}]},{"cell_type":"code","source":["# Test if model zoo loads correctly\n","from detectron2 import model_zoo\n","print(\"Model Zoo Import Successful\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2auEHaRCXTRz","outputId":"c8101539-4e88-48fe-bb7c-add199a5f506"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Zoo Import Successful\n"]}]},{"cell_type":"markdown","source":["Question 7: Annotate a dataset using any tool of your choice and convert the\n","annotations to COCO format for Detectron2"],"metadata":{"id":"8oNbKbTDXkBF"}},{"cell_type":"code","source":["from labelme import utils\n"],"metadata":{"id":"WZUorchHeDdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","os.makedirs(\"dataset/images\", exist_ok=True)\n","os.makedirs(\"dataset/annotations\", exist_ok=True)\n"],"metadata":{"id":"pofcDVaaeQfw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","from labelme import utils\n","import numpy as np\n","from PIL import Image\n","\n","input_dir = \"dataset/annotations\"\n","output_file = \"dataset/annotations/coco_train.json\"\n","\n","coco = {\n","    \"images\": [],\n","    \"annotations\": [],\n","    \"categories\": [{\"id\": 1, \"name\": \"tiger\"}, {\"id\": 2, \"name\": \"deer\"}] # example\n","}\n","\n","annotation_id = 1\n","for idx, file in enumerate(os.listdir(input_dir)):\n","    if file.endswith(\".json\"):\n","        path = os.path.join(input_dir, file)\n","        with open(path) as f:\n","            data = json.load(f)\n","\n","        # image info\n","        image_path = os.path.join(\"dataset/images\", data[\"imagePath\"])\n","        img = Image.open(image_path)\n","        width, height = img.size\n","\n","        coco[\"images\"].append({\n","            \"id\": idx + 1,\n","            \"file_name\": data[\"imagePath\"],\n","            \"width\": width,\n","            \"height\": height\n","        })\n","\n","        # object annotations\n","        for shape in data[\"shapes\"]:\n","            points = shape[\"points\"]\n","            x_min = min([p[0] for p in points])\n","            y_min = min([p[1] for p in points])\n","            x_max = max([p[0] for p in points])\n","            y_max = max([p[1] for p in points])\n","            width_box = x_max - x_min\n","            height_box = y_max - y_min\n","\n","            category_name = shape[\"label\"]\n","            category_id = next(c[\"id\"] for c in coco[\"categories\"] if c[\"name\"] == category_name)\n","\n","            coco[\"annotations\"].append({\n","                \"id\": annotation_id,\n","                \"image_id\": idx + 1,\n","                \"category_id\": category_id,\n","                \"bbox\": [x_min, y_min, width_box, height_box],\n","                \"area\": width_box * height_box,\n","                \"iscrowd\": 0\n","            })\n","            annotation_id += 1\n","\n","# Save COCO JSON\n","with open(output_file, \"w\") as f:\n","    json.dump(coco, f)\n"],"metadata":{"id":"u2eN2ki8ddwD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8. Write a script to download pretrained weights and configure paths for\n","training in Detectron2.\n"],"metadata":{"id":"lFvH4oRCYOZf"}},{"cell_type":"code","source":["# Import Detectron2 modules\n","from detectron2.config import get_cfg\n","from detectron2 import model_zoo\n","\n","# -----------------------------\n","# 1. Load base configuration\n","# -----------------------------\n","cfg = get_cfg()\n","\n","# Use a pretrained Faster R-CNN model from Detectron2 model zoo\n","config_file = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n","cfg.merge_from_file(model_zoo.get_config_file(config_file))\n","\n","# -----------------------------\n","# 2. Download pretrained weights\n","# -----------------------------\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)\n","print(\"Pretrained weights downloaded from:\", cfg.MODEL.WEIGHTS)\n","\n","# -----------------------------\n","# 3. Configure dataset paths\n","# -----------------------------\n","cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n","cfg.DATASETS.TEST = (\"my_dataset_val\",)\n","\n","# Image + annotation paths\n","cfg.DATASETS.TRAIN_IMG_DIR = \"dataset/train/images/\"\n","cfg.DATASETS.TRAIN_JSON = \"dataset/train/annotations.json\"\n","\n","cfg.DATASETS.VAL_IMG_DIR = \"dataset/val/images/\"\n","cfg.DATASETS.VAL_JSON = \"dataset/val/annotations.json\"\n","\n","# -----------------------------\n","# 4. Configure training settings\n","# -----------------------------\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.00025\n","cfg.SOLVER.MAX_ITER = 3000\n","\n","# Number of classes (change according to dataset)\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n","\n","# Output directory for saving weights and logs\n","cfg.OUTPUT_DIR = \"./output_model\"\n","\n","print(\"Training configuration completed!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpHef17uXdxo","outputId":"aa771d5a-d0e5-41dc-b019-df4b2b1308c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained weights downloaded from: https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\n","Training configuration completed!\n"]}]},{"cell_type":"markdown","source":["Question 9: Show the steps and code to run inference using a trained Detectron2\n","model on a new image.\n"],"metadata":{"id":"B55BQmewYluW"}},{"cell_type":"code","source":["import cv2\n","import torch\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog\n"],"metadata":{"id":"0xHRX_ZkYx6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","print(os.listdir())\n"],"metadata":{"id":"hFOlsB6aauHE","outputId":"728a6161-d3e8-4605-f262-6c1c4801bec9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['.config', 'sample_data']\n"]}]},{"cell_type":"code","source":["with open(\"config.yaml\", \"w\") as f:\n","    f.write(cfg.dump())\n"],"metadata":{"id":"uPJ6Eyu2a2VP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import os\n","print(os.listdir())\n"],"metadata":{"id":"KzkvXyeibGir","outputId":"840baab1-ecf5-42da-ff78-b1a26599b9c9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['.config', 'config.yaml', 'sample_data']\n"]}]},{"cell_type":"code","source":["cfg.OUTPUT_DIR = \"./output_model\"\n","import os\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n"],"metadata":{"id":"jHf2qkLmbPzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cfg.MODEL.DEVICE = \"cpu\"  # Force CPU training/inference\n"],"metadata":{"id":"F8Ft_W0ubXfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from detectron2.data.datasets import register_coco_instances\n","\n","\n","register_coco_instances(\n","    \"my_dataset_train\",\n","    {},\n","    \"dataset/annotations/coco_train.json\",\n","    \"dataset/images/train\"\n",")\n","\n","register_coco_instances(\n","    \"my_dataset_val\",\n","    {},\n","    \"dataset/annotations/coco_val.json\",\n","    \"dataset/images/val\"\n",")\n"],"metadata":{"id":"WM2NlqjdbnrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n","cfg.DATASETS.TEST = (\"my_dataset_val\",)\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # number of classes in your dataset\n","cfg.OUTPUT_DIR = \"./output_model\"\n","\n","import os\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n"],"metadata":{"id":"ARhzqa8Eb4Sr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from detectron2.data.datasets import register_coco_instances\n","\n","register_coco_instances(\n","    \"balloon_train\",\n","    {},\n","    \"dataset/annotations/balloon_train.json\",\n","    \"dataset/images/train\"\n",")\n","register_coco_instances(\n","    \"balloon_val\",\n","    {},\n","    \"dataset/annotations/balloon_val.json\",\n","    \"dataset/images/val\"\n",")\n"],"metadata":{"id":"RPLFRpQOb-8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cfg.DATASETS.TRAIN = (\"balloon_train\",)\n","cfg.DATASETS.TEST = (\"balloon_val\",)\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only 1 class: balloon\n","cfg.OUTPUT_DIR = \"./output_model\"\n"],"metadata":{"id":"-lucH-kfcLxZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 10: You are assigned to build a wildlife monitoring system to detect and track\n","different animal species in a forest using Detectron2. Describe the end-to-end pipeline\n","from data collection to deploying the model, and how you would handle challenges like\n","occlusion or nighttime detection."],"metadata":{"id":"yQ2O8frocYDY"}},{"cell_type":"code","source":["from detectron2.data.datasets import register_coco_instances\n","\n","register_coco_instances(\"wildlife_train\", {}, \"annotations/coco_train.json\", \"images/train\")\n","register_coco_instances(\"wildlife_val\", {}, \"annotations/coco_val.json\", \"images/val\")\n"],"metadata":{"id":"_gp0bzlzcPXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cfg.MODEL.DEVICE = \"cpu\"  # Force CPU usage\n"],"metadata":{"id":"sswifzVWcqrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["register_coco_instances(\n","    \"wildlife_train_v2\", {},\n","    \"dataset/annotations/coco_train.json\",\n","    \"dataset/images/train\"\n",")\n","register_coco_instances(\n","    \"wildlife_val_v2\", {},\n","    \"dataset/annotations/coco_val.json\",\n","    \"dataset/images/val\"\n",")\n","\n","cfg.DATASETS.TRAIN = (\"wildlife_train_v2\",)\n","cfg.DATASETS.TEST = (\"wildlife_val_v2\",)\n"],"metadata":{"id":"xRZw-UAHc3uD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from detectron2.data import DatasetCatalog\n","print(DatasetCatalog.list())\n"],"metadata":{"id":"nQ6kw7fWdA3K","outputId":"030e336a-d673-48d4-c92a-3b2e974a7e7c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['coco_2014_train', 'coco_2014_val', 'coco_2014_minival', 'coco_2014_valminusminival', 'coco_2017_train', 'coco_2017_val', 'coco_2017_test', 'coco_2017_test-dev', 'coco_2017_val_100', 'keypoints_coco_2014_train', 'keypoints_coco_2014_val', 'keypoints_coco_2014_minival', 'keypoints_coco_2014_valminusminival', 'keypoints_coco_2017_train', 'keypoints_coco_2017_val', 'keypoints_coco_2017_val_100', 'coco_2017_train_panoptic_separated', 'coco_2017_train_panoptic_stuffonly', 'coco_2017_train_panoptic', 'coco_2017_val_panoptic_separated', 'coco_2017_val_panoptic_stuffonly', 'coco_2017_val_panoptic', 'coco_2017_val_100_panoptic_separated', 'coco_2017_val_100_panoptic_stuffonly', 'coco_2017_val_100_panoptic', 'lvis_v1_train', 'lvis_v1_val', 'lvis_v1_test_dev', 'lvis_v1_test_challenge', 'lvis_v0.5_train', 'lvis_v0.5_val', 'lvis_v0.5_val_rand_100', 'lvis_v0.5_test', 'lvis_v0.5_train_cocofied', 'lvis_v0.5_val_cocofied', 'cityscapes_fine_instance_seg_train', 'cityscapes_fine_sem_seg_train', 'cityscapes_fine_instance_seg_val', 'cityscapes_fine_sem_seg_val', 'cityscapes_fine_instance_seg_test', 'cityscapes_fine_sem_seg_test', 'cityscapes_fine_panoptic_train', 'cityscapes_fine_panoptic_val', 'voc_2007_trainval', 'voc_2007_train', 'voc_2007_val', 'voc_2007_test', 'voc_2012_trainval', 'voc_2012_train', 'voc_2012_val', 'ade20k_sem_seg_train', 'ade20k_sem_seg_val', 'my_dataset_train', 'my_dataset_val', 'balloon_train', 'balloon_val', 'wildlife_train', 'wildlife_val', 'wildlife_train_v2', 'wildlife_val_v2']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wb6X1tFTdEo1"},"execution_count":null,"outputs":[]}]}